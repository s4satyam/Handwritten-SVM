##Problem Understanding####
##The images are flattend into csv and one row represent one image.
##we have to classify images which is a multiclass problem.
##we have to use svm techniques and best kernel for this problem.
##Comparison for all the kernels is to be provided


###Note:
##
##i havent scaled the data set as all columns varies from range 0-255.No Particular column is driving the output
## second i havent removed all zeros columns as If in test data digit is not in center 
##then it might then that column might be zero for training but it 
## might be non-zero in test.
## mnist data is clean and it has no left and right shifts or rotation
## But taking care of all these i havent removed columns with all zeros values in train and test


###As test data usually dont have labels/outputs so i have validated on mine test set generated by sampling after undersampling 
##mnist train data(10%).
##so overall validated it two times.
## used doParallel library for fast computation using multi cores

#####Pakages required#################
library(caret)
library(kernlab)
library(readr)
library(doParallel)
options(warn=-1)
##Removing scale Warning######
data1=read.csv('mnist_train.csv',header = F)
###gettting the train data
test=read.csv('mnist_test.csv',header = F)
##getting the train data


colnames(data1)[1]='Label'
colnames(test)[1]='Label'
##making the column name of all output variable as 'Label'
sapply(data1, function(x) sum(is.na(x)))
sapply(test, function(x) sum(is.na(x)))
##no na values in both train and test

##checking if dataset is balanced or not for both train and test file
library(plyr)
count(data1$Label)
##This data set is almost bablanced for multiclass classification
count(test$Label)
##test dataset is also balanced
##using 10% of the data only for buiding model for less compiling time##
##making target variable as factor for both train and test file
data1$Label=as.factor(data1$Label)
test$Label=as.factor(test$Label)
##duplication doesnt make sense here as they is no ID column.


###Sampling for using 10% of train data for trainin purpose

set.seed(999)
indices=sample(1:nrow(data1),0.1*nrow(data1))
sample_data=data1[indices,]
unused_data=data1[-indices,]

###Sample data will be used for training the model#####

#################################################################################################
##Training and Testing set  svm################
set.seed(1000)
indices=sample(1:nrow(sample_data),0.8*nrow(sample_data))
train=sample_data[indices,]####this data will be used for training all models
test1=sample_data[-indices,]###this data will be used for validation
##############################################################Linear Svm################################################################
model_1<- ksvm(Label ~ ., data = train,scale = F, kernel = "vanilladot")
model_1
# Predicting the model results on test1(used for validation)
evaluate_1<- predict(model_1, test1)

#Confusion Matrix - Finding accuracy, Sensitivity and specificity
confusionMatrix(evaluate_1, test1$Label)
##with c is =1(default) the model accuracy is 88.17% in validation
#############evaluating o main test set#######################
evaluate_2= predict(model_1,test)

confusionMatrix(evaluate_2,test$Label)
#on real test test set accuracy is 91.26%##################################################
##lets do cv now ####################################################to tune hyperparameters
trainControl <- trainControl(method="cv", number=5)
# Number - Number of folds 
# Method - cross validation

metric <- "Accuracy"

set.seed(100)
# making a grid of C values. 
grid <- expand.grid(C=c(0.1,0.2,0.6,2,5))

registerDoParallel()
# Performing 5-fold cross validation
svm_linear <- train(Label~., data=train, method="svmLinear", metric=metric, 
                 tuneGrid=grid, trControl=trainControl)

# Printing cross validation result
print(svm_linear)
# Best tune at C=0.1, 
# Accuracy - 89.41%

# Plotting "fit.svm" results
plot(svm_linear)


# Valdiating the model after cross validation on test data
evaluate_linear_test1<- predict(svm_linear, test1)
confusionMatrix(evaluate_linear_test1, test1$Label)
##accuracy we are getting on validation is 88.17%


##real test set accuracy
evaluate_linear_test<- predict(svm_linear, test)
confusionMatrix(evaluate_linear_test, test$Label)
### We are getting total accuracy on test set 91.26% aftre CV
###Now for linear model we will will use svm_linear as it is giving 91.26% accuracy with C=1###
###########################################################Poly Kernel##########################################
##Now lets try with other kernels if we can increase accuracy####

###Lets look for Poly Kenerls how do they behave################################
model_2 <- ksvm(Label ~ ., data =train,scale=FALSE, kernel = "polydot",kpar=list(degree=2),C=1)
model_2
evaluate_degree_2_test1<- predict(model_2, test1)
confusionMatrix(evaluate_degree_2_test1, test1$Label)
### accuracy improved from linear default kernel slightly its around 94%
##now lets try on real test set
evaluate_degree_2_test<- predict(model_2, test)
confusionMatrix(evaluate_degree_2_test, test$Label)
### Here also accuracy is slightly above than 95%


##Now lets tune the hyper parameters for this
set.seed(19)
grid <- expand.grid(degree=c(2,3,4,5), C= c(0.1,0.2,0.3,2,5),scale=0.1)


fit_poly <- train(Label~., data=train, method="svmPoly", metric=metric,tuneGrid=grid, trControl=trainControl)
warnings()
print(fit_poly)
plot(fit_poly)
###The final values used for the model were degree = 2, scale = 0.1 and C = 0.1.

##Now testing on test1 data set after splitting

eval_Poly_2=predict(fit_poly,test1[,-1])
confusionMatrix(eval_Poly_2,test1$Label)
### Accuracy is 93.42 % on test1 after spllitting


##now lets check on whole test set

eval_Poly_2_test=predict(fit_poly,test[,-1])
confusionMatrix(eval_Poly_2_test,test$Label)
## accuracy is 95.03% on the test set 
#############In poly kernel accuracy improved from 91 to 95% on the test set#########################################
##Now lets check if Radial kernel can be more accurate while it doesn't overfit as we have already added non-linearity 
###################################################################################################################

model_3 <- ksvm(Label ~ ., data =train,scale=0.1, kernel = "rbfdot",C=1)
# Predicting the model results 
Eval_RBF<- predict(model_3, test1)
#model_3
#confusion matrix - RBF Kernel
confusionMatrix(Eval_RBF,test1$Label)
##we can see with default parameters we got 92.42% accuracy on test set after split
##now lets check on real test set
Eval_RBF_test<- predict(model_3, test)
#confusion matrix - RBF Kernel
confusionMatrix(Eval_RBF_test,test$Label)
## we got 95.04 % accuracy on real test set with default parameters.
## now lets try if can we can improve using Cross validation and parameter tuning in Radial Kernel
###################################################################################################################
 
trainControl <- trainControl(method="cv", number=5)
metric <- "Accuracy"
set.seed(9999)
#here i have used tune length in comparsion to tune grid as it is computational faster and it gives 
## single sigma and the most accurate c value
library(doParallel)
registerDoParallel()
fit.svm1 <- train(Label~., data=train, method="svmRadial", metric=metric, tunelength=5,trControl=trainControl)
print(fit.svm1)
plot(fit.svm1)
fit.svm1
##now we had optimal C which is 1 and 
##now lets try to validate through sigma value 1.644029e-07 from this method
Eval_RBF_test1_cv<- predict(fit.svm1, test1)
#confusion matrix - RBF Kernel
confusionMatrix(Eval_RBF_test1_cv,test1$Label)
##getting 92.27% validation
Eval_RBF_test_cv<- predict(fit.svm1, test)
#confusion matrix - RBF Kernel
confusionMatrix(Eval_RBF_test_cv,test$Label)
### here we are also get 95.05% accuracy

############Conclusion##########################################################################################################
##linear kernel was giving 91% accuracy on test set which is good######
##poly kernel was giving 95.04% accuracy which degree-2 so adding slightly non-linearity accuracy improves
##significantly for same C value justified by Cross-validation
##radial kernel is having very less sigma value employing that and C=1 employing that not much amount of non-linearity in
##this model and accuracy also is 95.05% almost same as degree-2 poly kernel.
###So, on whole data is having some amount of non-linearity but not so much non-linearity and Poly(degree-2) model being 
##having optimal bias and variance as its bias is less  than linear(accuracy) and variance(less complex) is less than radial kernel
##with same amount 
##of accuracy.






